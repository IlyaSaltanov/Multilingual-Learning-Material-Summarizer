"""
–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã
"""

import os
import nltk
import urllib.request
import tempfile
import zipfile
import shutil
import ssl
from typing import List, Tuple

def disable_ssl():
    """–û—Ç–∫–ª—é—á–∞–µ–º SSL –ø—Ä–æ–≤–µ—Ä–∫—É"""
    try:
        _create_unverified_https_context = ssl._create_unverified_context
    except AttributeError:
        pass
    else:
        ssl._create_default_https_context = _create_unverified_https_context

def setup_nltk():
    """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º NLTK –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç"""
    print("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ NLTK...")
    
    disable_ssl()  # –û—Ç–∫–ª—é—á–∞–µ–º SSL –¥–ª—è –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö
    try:
        nltk.data.find('tokenizers/punkt')
        print("‚úÖ NLTK –¥–∞–Ω–Ω—ã–µ —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
        return True
    except LookupError:
        print("üì• NLTK –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –ø—Ä–æ–±—É—é —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å...")
    
    try:
        # –ü—Ä–æ–±—É–µ–º —Å–∫–∞—á–∞—Ç—å
        print("–°–∫–∞—á–∏–≤–∞—é punkt...")
        nltk.download('punkt', quiet=True)
        
        # –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∏ –Ω–µ–º–µ—Ü–∫–æ–≥–æ
        try:
            nltk.download('punkt_tab', quiet=True)
        except:
            print("‚ö†Ô∏è  punkt_tab –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É—é —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π punkt")
        
        print("‚úÖ NLTK –¥–∞–Ω–Ω—ã–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
        return True
        
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ NLTK: {e}")
        print("–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å fallback —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é")
        return False

def simple_tokenize(text: str) -> List[str]:
    """–ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –µ—Å–ª–∏ NLTK –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç"""
    sentences = []
    current = []
    
    for char in text:
        current.append(char)
        if char in '.!?„ÄÇÔºÅÔºü':
            sentence = ''.join(current).strip()
            if sentence:
                sentences.append(sentence)
            current = []
    
    if current:
        sentences.append(''.join(current).strip())
    
    return [s for s in sentences if s]

def validate_text_length(text: str, min_len: int = 50, max_len: int = 10000) -> Tuple[bool, str]:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞
    
    Returns:
        (is_valid, error_message)
    """
    length = len(text)
    
    if length < min_len:
        return False, f"Text too short (min {min_len} characters)"
    
    if length > max_len:
        return False, f"Text too long (max {max_len} characters)"
    
    return True, ""

def split_into_chunks(text: str, max_chunk_size: int = 5000) -> List[str]:
    """–†–∞–∑–¥–µ–ª—è–µ—Ç –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏"""
    words = text.split()
    chunks = []
    current_chunk = []
    current_size = 0
    
    for word in words:
        current_chunk.append(word)
        current_size += len(word) + 1
        
        if current_size >= max_chunk_size:
            chunks.append(' '.join(current_chunk))
            current_chunk = []
            current_size = 0
    
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    
    return chunks
