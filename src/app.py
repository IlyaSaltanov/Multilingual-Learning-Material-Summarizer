"""–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π Multilingual Summarizer –¥–ª—è MacBook."""

import os
import re
import ssl
from flask import Flask, render_template, request, jsonify
import nltk
from langdetect import detect, DetectorFactory

# –û—Ç–∫–ª—é—á–∞–µ–º SSL –ø—Ä–æ–≤–µ—Ä–∫—É –¥–ª—è NLTK (—Ä–µ—à–µ–Ω–∏–µ –¥–ª—è Mac)
try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

# –î–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —è–∑—ã–∫–∞
DetectorFactory.seed = 42


def download_nltk_data():
    """–°–∫–∞—á–∏–≤–∞–µ–º NLTK –¥–∞–Ω–Ω—ã–µ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ (—Å SSL bypass)."""
    try:
        nltk.data.find("tokenizers/punkt")
        print("‚úÖ NLTK –¥–∞–Ω–Ω—ã–µ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    except LookupError:
        print("üì• –ó–∞–≥—Ä—É–∂–∞—é NLTK –¥–∞–Ω–Ω—ã–µ...")
        try:
            nltk.download("punkt", quiet=True)
            nltk.download("punkt_tab", quiet=True)
            print("‚úÖ NLTK –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ NLTK –¥–∞–Ω–Ω—ã—Ö: {e}")
            print("–ü–æ–ø—Ä–æ–±—É—é –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥...")
            # –ü–æ–ø—Ä–æ–±—É–µ–º —Å–∫–∞—á–∞—Ç—å –≤—Ä—É—á–Ω—É—é
            import urllib.request
            import tempfile
            import zipfile
            import shutil

            # –°–∫–∞—á–∏–≤–∞–µ–º punkt –Ω–∞–ø—Ä—è–º—É—é
            punkt_url = "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/tokenizers/punkt.zip"
            temp_dir = tempfile.mkdtemp()

            try:
                # –°–∫–∞—á–∏–≤–∞–µ–º –∞—Ä—Ö–∏–≤
                print("–°–∫–∞—á–∏–≤–∞—é punkt –Ω–∞–ø—Ä—è–º—É—é...")
                urllib.request.urlretrieve(
                    punkt_url, os.path.join(temp_dir, "punkt.zip")
                )

                # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º
                with zipfile.ZipFile(
                    os.path.join(temp_dir, "punkt.zip"), "r"
                ) as zip_ref:
                    zip_ref.extractall(temp_dir)

                # –ö–æ–ø–∏—Ä—É–µ–º –≤ –ø–∞–ø–∫—É nltk_data
                nltk_data_dir = os.path.expanduser("~/nltk_data")
                tokenizers_dir = os.path.join(nltk_data_dir, "tokenizers")

                os.makedirs(tokenizers_dir, exist_ok=True)

                # –ò—â–µ–º —Ñ–∞–π–ª—ã punkt
                for root, dirs, files in os.walk(temp_dir):
                    for file in files:
                        if "punkt" in file and file.endswith(".pickle"):
                            src = os.path.join(root, file)
                            dst = os.path.join(tokenizers_dir, file)
                            shutil.copy2(src, dst)
                            print(f"–°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω: {file}")

                print("‚úÖ NLTK –¥–∞–Ω–Ω—ã–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –≤—Ä—É—á–Ω—É—é")
            except Exception as e2:
                print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å NLTK –¥–∞–Ω–Ω—ã–µ: {e2}")
                print("–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å fallback —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é")


# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
download_nltk_data()

# –£–∫–∞–∑—ã–≤–∞–µ–º —è–≤–Ω—ã–π –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ templates (–Ω–∞ —É—Ä–æ–≤–µ–Ω—å –≤—ã—à–µ –≤ src/)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
TEMPLATE_DIR = os.path.join(BASE_DIR, "../templates")

app = Flask(__name__, template_folder=TEMPLATE_DIR)

# –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —è–∑—ã–∫–∏
SUPPORTED_LANGUAGES = {"en": "English", "ru": "Russian", "de": "German"}


def simple_tokenize(text):
    """–ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –µ—Å–ª–∏ NLTK –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç."""
    # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ —Ç–æ—á–∫–∞–º, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º –∑–Ω–∞–∫–∞–º
    sentences = []
    current_sentence = []

    for char in text:
        current_sentence.append(char)
        if char in ".!?„ÄÇÔºÅÔºü":
            sentences.append("".join(current_sentence).strip())
            current_sentence = []

    if current_sentence:
        sentences.append("".join(current_sentence).strip())

    return [s for s in sentences if s]


def detect_language_simple(text):
    """–ü—Ä–æ—Å—Ç–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞."""
    if len(text.strip()) < 20:
        return {"language": "en", "confidence": 0.5}

    try:
        detected = detect(text)
        if detected in SUPPORTED_LANGUAGES:
            return {"language": detected, "confidence": 0.9}
    except Exception:
        pass

    # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
    text_lower = text.lower()
    ru_chars = set("–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è")
    de_chars = set("√§√∂√º√ü")

    ru_count = sum(1 for c in text_lower[:200] if c in ru_chars)
    de_count = sum(1 for c in text_lower[:200] if c in de_chars)

    if ru_count > 10:
        return {"language": "ru", "confidence": min(0.9, ru_count / 100)}
    elif de_count > 5:
        return {"language": "de", "confidence": min(0.9, de_count / 50)}

    return {"language": "en", "confidence": 0.5}


def select_sentences_for_summary(cleaned_sentences, target_sentences):
    """–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–±–æ—Ä–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏."""
    result_sentences = []

    if not cleaned_sentences:
        return result_sentences

    # –í—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–∞–µ–º –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
    result_sentences.append(cleaned_sentences[0])

    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    if len(cleaned_sentences) > 3:
        middle_idx = len(cleaned_sentences) // 2
        if middle_idx < len(cleaned_sentences) and len(result_sentences) < target_sentences:
            result_sentences.append(cleaned_sentences[middle_idx])

    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å –º–µ—Å—Ç–æ
    if len(cleaned_sentences) > 1 and len(result_sentences) < target_sentences:
        result_sentences.append(cleaned_sentences[-1])

    # –î–æ–±–∞–≤–ª—è–µ–º –¥—Ä—É–≥–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if len(result_sentences) < target_sentences and len(cleaned_sentences) > 2:
        for i in range(1, len(cleaned_sentences) - 1):
            if i != middle_idx and len(result_sentences) < target_sentences:
                result_sentences.append(cleaned_sentences[i])

    return result_sentences[:target_sentences]


def summarize_text_extractive(text, language, compression_percent):
    """–ü—Ä–æ—Å—Ç–æ–π extractive summarizer."""
    sentences = []

    try:
        # –ü—Ä–æ–±—É–µ–º NLTK —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é
        if language == "ru":
            sentences = nltk.sent_tokenize(text, language="russian")
        elif language == "de":
            sentences = nltk.sent_tokenize(text, language="german")
        else:
            sentences = nltk.sent_tokenize(text, language="english")
    except Exception:
        # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç—É—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é
        sentences = simple_tokenize(text)

    if len(sentences) <= 3:
        return text

    # –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    cleaned_sentences = []
    for sentence in sentences:
        clean_sentence = re.sub(r"\s+", " ", sentence).strip()
        if len(clean_sentence.split()) > 3:
            cleaned_sentences.append(sentence)

    if not cleaned_sentences:
        return text

    # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã
    target_sentences = max(
        2, int(len(cleaned_sentences) * (compression_percent / 100)))

    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
    result_sentences = select_sentences_for_summary(
        cleaned_sentences, target_sentences)

    return " ".join(result_sentences)


@app.route("/")
def home():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞."""
    return render_template("index.html")


@app.route("/summarize", methods=["POST"])
def summarize():
    """API endpoint –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏."""
    try:
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        data = request.get_json()
        if not data or "text" not in data:
            return jsonify({"error": "No text provided"}), 400

        text = data["text"].strip()

        if len(text) < 50:
            return jsonify({"error": "Text too short (min 50 characters)"}), 400

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫
        language = data.get("language", "auto")
        if language == "auto":
            detected = detect_language_simple(text)
            language = detected["language"]
            confidence = detected["confidence"]
        else:
            confidence = 1.0

        # –ü–æ–ª—É—á–∞–µ–º —É—Ä–æ–≤–µ–Ω—å —Å–∂–∞—Ç–∏—è
        compression = data.get("compression", 30)
        if compression not in [20, 30, 50]:
            compression = 30

        # –°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
        summary = summarize_text_extractive(text, language, compression)

        # –°—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        original_words = len(text.split())
        summary_words = len(summary.split())

        response = {
            "success": True,
            "summary": summary,
            "language": language,
            "language_name": SUPPORTED_LANGUAGES.get(language, "Unknown"),
            "confidence": round(confidence, 2),
            "compression": compression,
            "original_length": original_words,
            "summary_length": summary_words,
            "reduction": round((1 - summary_words / original_words) * 100, 1)
            if original_words > 0
            else 0,
        }

        return jsonify(response)

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/health")
def health():
    """Health check endpoint."""
    return jsonify({"status": "healthy", "service": "Multilingual Summarizer"})


if __name__ == "__main__":
    port = int(os.environ.get("PORT", 5000))
    debug = os.environ.get("FLASK_ENV") == "development"

    print(f"üöÄ –ó–∞–ø—É—Å–∫ Multilingual Summarizer –Ω–∞ –ø–æ—Ä—Ç—É {port}")
    print(f"üåê –û—Ç–∫—Ä–æ–π—Ç–µ http://localhost:{port} –≤ –±—Ä–∞—É–∑–µ—Ä–µ")
    print("üìù –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —è–∑—ã–∫–∏: English, Russian, German")
    print(f"üìÅ –ü—É—Ç—å –∫ —à–∞–±–ª–æ–Ω–∞–º: {TEMPLATE_DIR}")

    app.run(host="0.0.0.0", port=port, debug=debug)
